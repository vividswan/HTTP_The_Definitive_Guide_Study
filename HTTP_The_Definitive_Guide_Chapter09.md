# 9장 - 웹 로봇

- 웹 로봇
  - 스스로 움직이는 사용자 에이전트
  - 사람과 상호작용 X
  - 웹 트랜잭션들 자동 수행
  - 크롤러, 스파이더, 웜, 봇 ...

## 9.1 크롤러와 크롤링

- 크롤러 혹은 스파이더로 불림
- 웹페이지를 가져오고 재귀적으로 그 웹페이지가 가리키는 모든 웹페이지를 가져옴
- 검색엔진들이 크롤러로 모든 문서를 끌어오고 처리 후 검색 가능한 데이터베이스로 만듦
  - 특정 단어를 포함한 문서를 찾을 수 있게 해줌

### 9.1.1 어디에서 시작하는가: '루트 집합'

- 크롤러를 사용하기 전 출발 지점을 주어야 함
  - 크롤러가 방문을 시작하는 URL들의 초기 집합이 루트 집합(root set)
  - 충분히 다른 장소에서 루트 집합을 선택할 것
- 루트 집합에 너무 많은 페이지가 있을 필요는 없음
  - 크고 인기 있는 웹 사이트
  - 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록
- 대규모 크롤러 제품들은 사용자들에게 루트 집합에 페이지 추가하는 기능 제공

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러들이 발견하는 새 링크들은 급속히 확장
  - 간단한 HTML 파싱 후 상대 링크를 절대 링크로 반환해야 함

### 9.1.3 순환 피하기

- 웹 크롤링 중 루프나 순환에 빠지지 않도록 조심
- 로봇들은 그들이 어디를 방문했는지 알아야 함

### 9.1.4 루프와 중복

- 루프는 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 할 수 있음
  - 크롤러가 네트워크 대역폭을 다 차지하게 돼버림
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 됨
  - 웹 사이트 압박 후 어떤 실제 사용자도 접근할 수 없게 막아버릴 수 있음
- 많은 수의 중복된 페이지를 가져오면 애플리케이션이 중복된 콘텐츠로 넘쳐나게 될 것

### 9.1.5 빵 부스러기의 흔적

- 인터넷에선 수십억 개의 서로 다른 웹페이지들이 존재
  - 방문한 곳을 지속적으로 추적하는 것이 어려움
- 방문 판단을 위해 속도와 메모리 사용이 효과적인 자료구조 필요
  - 검색 트리, 해시테이블이 필요
  - 수억 개의 URL은 많은 공간을 차지
- 트리와 해시 테이블
  - URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료구조
- 느스한 존재 비트맵
  - 공간 사용을 최소화하기 위한 존재 비트 배열
  - 각 URL이 해시 함수에 의해 고정된 크기와 숫자로 변환되고 배열 안에 대응하는 존재 비트를 가짐
  - 존재 비트로 URL의 크롤링 여부 확인
- 체크포인트
  - 중단될 경우를 대비해서 방문한 URL의 목록이 디스크에 저장되었는지 확인
- 파티셔닝
  - 크롤링을 완수하기엔 한 대의 컴퓨터론 부족
  - 각각이 분리된 로봇들이 동시에 일하는 농장을 이용
  - 각 로봇엔 URL의 특정 '한 부분'이 할당

### 9.1.6 별칭(alias)과 로봇 순환

- 별칭에 의해 서로 달라 보여도 같은 리소스를 가리키고 있을 때가 있음

### 9.1.7 URL 정규화하기

- 웹 로봇들은 정규화를 통해 같은 리소스를 가리키고 있는 것들은 미리 제거
- 정규화된 형식으로 변환하는 방식
  - 포트 번호가 없을 시 호스트 명에 ':80' 추가
  - 모든 %xx 이스케이핑 된 문자들은 대응되는 문자로 변환
  - \# 태그들을 제거
- 정규화 방식의 한계
  - 대소문자 구분 여부
  - 색인 페이지 설정을 모름
  - 웹 서버의 가상 호스팅 설정 여부

### 9.1.8 파일 시스템 링크 순환

- 심벌릭 링크는 교묘한 종류의 순환을 유발 가능
  - 함정에 빠지도록 악의적으로 만들기도 함
- URL이 달라 보이지만 순환이 되는 파일 시스템 링크는 로봇을 루프로 빠져들게 하는 위험이 있음

### 9.1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들이 크롤러 루프를 만드는 것은 있을 수 있는 일
- ex)
  - 평범한 파일처럼 보이지만 게이트웨이 애플리케이션인 URL 생성
  - 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정(CGI 기반의 달력 프로그램)

### 9.1.10 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없음
  - 잘 설계된 로봇은 휴리스틱 집합을 필요
- 더욱 자율적인 크롤러는 문제를 피하는 데 도움을 주지만 손실도 유발 가능
- 로봇이 더 올바르게 웹에서 작동하기 위한 여러 가지 기법들이 존재
- URL 정규화
  - URL을 표준 형태로 변환
- 너비 우선 크롤링
  - URL의 큰 집합을 너비 우선으로 스케줄링
  - 순환의 영향 최소화
  - 순화에서 페이지를 받아오기 전 다른 웹 사이트에서 수십만 개의 페이지 받아올 수 있음
  - 깊이 우선 스케줄링은 순환을 건드리는 경우 빠져나올 수 없음
- 스로틀링
  - 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지 숫자를 제한
  - 순환에 빠졌을 시 총 횟수 제한 가능
- URL 크기 제한
  - 로봇은 일정 길이를 넘는 URL의 크롤링은 거부 가능
  - 순환 시 늘어나는 길이 제한으로 인해 순환이 중단될 수 있음
  - 이 기법을 사용 시 가져오지 못하는 콘텐츠 발생 가능
- URL/사이트 블랙리스트
  - 순환 및 함정인 URL의 목록을 만들어 관리
  - 문제 시 블랙리스톨 추가
  - 사람의 손을 필요로 함
- 패턴 발견
  - 파일 시스템의 오설정들은 일정 패턴을 따르는 경향이 있음
  - 로봇은 몇 가지 다른 주기의 반복 패턴을 감지해냄
- 콘텐츠 지문(fingerprint)
  - 중복 감지 보다 직접적인 방법
  - 콘텐츠에서 몇 바이트를 얻어내어 체크섬 계산
  - 동일한 체크섬은 크롤링 X
  - 체크섬은 내용이 다름에도 똑같을 확률이 적은 것을 사용 (MD5와 같은 메시지 요약 함수 등...)
  - 서버의 임베딩된 링크나 동적인 동작은 중복 감지를 방해할 수 있음
- 사람의 모니터링
  - 모든 상용 수준 로봇은 모니터링을 위한 진단과 로깅을 반드시 포함해야 함
  - 웹 크롤링을 위한 좋은 스파이더 휴리스틱을 만드는 작업은 언제나 현재진행형
  - 모니터링에 더 의존해서 일부 문제를 더 피해 갈 수 있는 커스터마이징된 크롤러들 존재

## 9.2 로봇의 HTTP

- 로봇들은 다른 HTTP 클라이언트 프로그램과 다르지 않음
  - HTTP 명세 규칙을 지켜야 함
  - 많은 로봇들은 HTTP를 최소한으로만 구현하려고 함 (문제 유발 가능)

### 9.2.1 요청 헤더 식별하기

- 로봇들은 약간의 신원 식별 헤더를 구현하고 전송
  - User-Agent : 서버에게 요청을 만든 로봇의 이름
  - From : 로봇의 사용자나 관리자의 이메일 주소
  - Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줌
  - Referer : 현재의 요청 URL을 포함한 문서의 URL을 제공

### 9.2.2 가상 호스팅

- HTTP/1.1은 Host 헤더를 사용할 것을 요구
  - Host 헤더가 없을 시 URL에 대해 잘못된 콘텐츠를 찾게 만듦
- Host 헤더가 없을 시 두 개의 사이트를 운영하는 서버의 페이지에 대한 요청에서 잘못된 콘텐츠를 크롤링이 얻을 수 있다.

### 9.2.3 조건부 요청

- 인터넷 검색엔진 로봇과 같은 경우 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미가 있음
  - 캐시의 유효성을 검사하는 방식과 유사한 조건부 HTTP 요청을 구현

### 9.2.4 응답 다루기

- HTTP의 특정 몇몇 기능을 사용하거나 웹 탐색, 서버와의 상호작용이 필요한 로봇들은 HTTP 응답을 다룰 줄 알아야 함
- 상태 코드
  - 최소한 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다뤄야 함 (200, 404)
  - 이해할 수 없는 상태 코드는 상태 코드가 속한 분류에 근거하여 다룰 것
  - 모든 서버가 언제나 적절한 에러 코드를 반환하지는 않음
- 엔터티
  - HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있음
  - http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보
  - 로봇 구현자들은 http-equiv 정보를 찾아내기 위해 HTML 문서의 HEAD 태그를 탐색하기를 원할 수도 있음

### 9.2.5 User-Agent 타기팅

- 웹 관리자들은 많은 로봇들로부터의 방문과 요청을 예상해야 함
- 많은 웹 사이트들이 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지 후 그에 맞게 최적화
- 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 함
  - 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록

## 9.3 부적절하게 동작하는 로봇들

- 폭주하는 로봇
  - 로봇은 사람보다 빠르게 HTTP 요청 생성 가능 & 빠른 네트워크 연결을 갖춘 컴퓨터 위에서 동작
  - 논리적인 에러나 순환에 빠지면 웹 서버에 극심한 부하
  - 폭주 방지를 위한 보호 장치를 반드시 신경 써서 설계해야 할 것
- 오래된 URL
  - 오래된 URL 접속으로 인해 존재하지 않는 URL에 요청을 많이 보낼 수 있음
  - 이는 에러 로그가 채워지거나 웹 서버의 부하가 늘어 날 수 있음
- 길고 잘못된 URL
  - 순환 및 프로그래밍 상의 오류로 크고 의미 없는 URL 요청을 할 수 있음
  - 웹 서버 처리 영향, 접근 로그가 의미 없이 채워지거나 고장 날 수 있음
- 호기심이 지나친 로봇
  - 로봇이 사적인 데이터에 대한 URL을 얻어 소유자가 원하지 않음에도 불구하고 쉽게 접근하도록 만들 수 있음
  - 민감한 데이터들을 그들의 로봇이 접근할 수 있다는 것을 인지해야 함
- 동적 게이트웨이 접근
  - 로봇이 게이트웨이 애플리케이션의 콘텐츠에 대한 URL을 요청할 수도 있음
  - 대부분의 이러한 데이터들은 비용이 많이 들어감

## 9.4 로봇 차단하기

- 로봇 커뮤니티에서 로봇에 대한 웹 사이트 접근에 대한 문제를 막기 위해 메커니즘에 대한 기법 제공
  - "Robots Exclusion Standard" 라고 이름 지어졌지만 종종 그냥 'robots.txt'라고 불림
- 서버가 문서 루트에 robots.txt를 제공하면 로봇이 다른 리소스에 접근하기 전 이를 요청하여 권한을 확인함

### 9.4.1 로봇 차단 표준

- 로봇 차단 표준은 임시방편으로 마련된 표준
  - 대부분의 주류 업체들과 검색 엔진 크롤러들은 차단 표준을 지원
- 세 가지 버전 존재
  - v0.0, v1.0, v2.0
  - 대부분의 로봇들은 v0.0, v1.0 채택, v2.0은 훨씬 복잡함

### 9.4.2 웹 사이트와 robots.txt 파일들

- robots.txt 파일이 존재할 시 로봇은 반드시 그 파일을 가져와서 처리해야 함
- 호스트 명과 포트 번호에 정의되는 하나의 웹 사이트엔 robots.txt가 단 하나 존재
- 가상 호스팅 될 시 가상 docroot에 서로 다른 robots.txt가 있을 수 있음
- robots.txt 가져오기
  - HTTP GET 메서드를 이용해 가져옴
  - robots.txt가 없을 시 404 리턴
  - 404를 받은 로봇은 접근 제한이 없는 것으로 간주하고 모든 요청
  - 로봇은 From이나 User-Agent 헤더를 통해 신원 정보를 넘김
- 응답 코드
  - 서버가 성공으로 응답하면 로봇은 응답의 콘텐츠를 파싱 후 차단 규칙을 얻음
  - 서버가 404 응답 시 로봇은 차단 규칙이 없다고 가정하고 제약 없이 사이트에 접근
  - 서버가 접근 제한(401, 403)으로 응답 시 로봇은 그 사이트로의 접근은 완전히 제한되어 있다고 가정
  - 503 응답과 함께 요청 시도가 일시적으로 실패했다면 로봇은 나중에 그 사이트의 리소스 검색
  - 서버 응답이 3XX의 리다이렉션을 의미하면 로봇은 리소스가 발견될 때까지 리다이렉트를 따라가야 함

### 9.4.3 robots.txt 파일 포맷

- 단순한 줄 기반 문법
- 빈 줄, 주석 줄, 규칙 줄의 세 가지 종류
- 규칙줄
  - HTTP 헤더처럼 생김
  - 패턴 매칭을 위해 사용
- robots.txt의 줄들은 레코드로 구분
  - 각 레코드는 특정 로봇들의 집합에 대한 차단 규칙의 집합을 기술
  - 빈 줄이나 파일 끝 문자로 끝냄
  - 영향을 받는 로봇을 알려주는 하나 이상의 User-Agent 줄로 시작
  - 로봇들이 접근할 수 있는 URL에 대한 Allow 줄과 Disallow 줄이 뒤이어 옴
- User-Agent 줄
  - User-Agent: <\robot-name>
  - 각 줄은 하나 이상의 User-Agent 줄로 시작
  - 로봇 이름이 자신 이름의 부분 문자열이 될 수 있는 레코드들 중 첫 번째 것이거나 와일드카드 일 때 복종해야 됨
- Disallow와 Allow 줄들
  - URL 경로에 대한 명시적 금지나 허용 기술
  - 로봇은 요청하려고 하는 URL을 두 규칙에 순서대로 맞춰 보아야 함
  - 첫 번째로 맞는 것이 사용
  - 어떤 것도 맞지 않을 시 그 URL은 허용
  - 규칙 경로는 반드시 맞춰보고자 하는 경로의 대소문자를 구분하는 접두어
- Disallow/Allow 접두 매칭(prefix matching)
  - 문자열이 경로와 같아야 함 (대소문자도), 별표는 빈 문자열로 모든 문자열과 매치되도록 할 수 있다.
  - 빗금을 제외한 임의의 이스케이핑 된 문자들은 비교 전에 원래 대로 복원됨
  - 규칙 경로가 빈 문자열일 시 모든 URL 경로와 매치

### 9.4.4 그 외에 알아둘 점

- robots.txt 파일은 명세가 발전함에 따라 다른 필드 포함 가능
- 한 줄을 여러 줄로 나누어 적는 것은 허용 X
- 주석은 파일의 어디에든 허용
  - 주석 문자 : '#', 줄바꿈 문자가 끝
- v0.0은 Allow 줄 지원 X

### 9.4.5 robots.txt 캐싱과 만료

- 매 파일 접근마다 robots.txt 파일을 가져오지 않고 주기적으로 robots.txt을 가져와서 결과를 캐싱
  - 부하 감소
- 표준 HTTP 캐시 제어 메커니즘이 원 서버와 로봇 양쪽 모두에 의해 사용
  - 로봇은 Cache-Control과 Expires 헤더에 주의를 기울여야 함

### 9.4.6 로봇 차단 펄 코드

- robots.txt와 상호작용하는 펄 라이브러리가 몇 가지 존재
- 한 예는 CPAN 공개 펄 아카이브의 WWW::RobotsRules 모듈
  - RobotRules 객체 만들기, robots.txt 파일 로드하기, 사이트 URL을 가져올 수 있는지 검사하기 등의 메서드가 있음

### 9.4.7 HTML 로봇 제어 META 태그

- robots.txt의 단점 : 개개인이 아닌 웹 사이트 관리자가 파일 소유
- HTML 문서에 직접 로봇 제어 태그를 추가할 수 있음
- NOINDEX 지시자
  - 로봇에게 이 페이지를 처리하지 말고 무시하라고 지시
  - <\META NAME="ROBOTS" CONTENT="NOINDEX">
- NOFOLLOW 지시자
  - 로봇에게 이 페이지가 링크한 페이지를 크롤링 하지 말라고 지시
  - <\META NAME="ROBOTS" CONTENT="NOFOLLOW">
- INDEX 지시자
  - 이 페이지의 인덱스 허용
- FOLLOW 지시자
  - 이 페이지가 링크한 페이지 크롤링 허용
- NOARCHIVE 지시자
  - 로봇에게 페이지의 캐시를 위한 로컬 사본을 못 만들게 지시
- ALL 지시자
  - INDEX, FOLLOW와 동일
- NONE 지시자
  - NOINDEX, NOFOLLOW와 동일
- 로봇 META 태그는 반드시 HTML의 HEAD 섹션에 나타나야 함
- 지시자 대소문자 구분 X, 서로 충돌되게 하지 말 것
- 검색엔진 META 태그
  - 모든 로봇 META 태그는 name="robot" 속성 포함
  - DESCRIPTION 태그 : 저자가 웹 페이지의 짧은 요약을 정의할 수 있게 해줌
  - KEYWORDS 태그 : 키워드 검색을 돕기 위한 웹페이지를 기술하는 단어들의 쉼표로 구분되는 목록
  - REVISIT-AFTER : 로봇이나 검색엔진에게 지정된 만큼의 날짜가 지난 이후 다시 방문하라고 지시할 수 있음

## 9.5 로봇 에티켓

- 신원 식별
  - 로봇의 신원을 밝히라 (User-agent로)
  - 기계의 신원을 밝히라 (역방향 DNS를 할 수 있도록)
  - 연락처를 밝히라 (HTTP 폼 필드를 통해)
- 동작
  - 긴장하라 (로봇이 24시간 올바르게 동작하는지)
  - 대비하라 (조직에게 로봇 사용을 공지)
  - 감시와 로그 (크롤러의 동작에 대한)
  - 배우고 조정하라 (매번 로봇을 개선 & 조정할 것)
- 스스로를 제한하라.
  - URL을 필터링하라 (찾던 것이 맞는지 확인)
  - 동적 URL을 필터링하라 (게이트웨이로부터 콘텐츠는 크롤링 X)
  - Accept 관련 헤더로 필터링 (헤더로 서버에게 말해줘야 함)
  - robots.txt에 따르라
  - 스스로를 억제하라 (특정 사이트에 너무 자주 방문 X)
- 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
  - 모든 응답 코드 다루기 (로그와 모니터링도)
  - URL 정규화 하기 (중복된 URL 제거)
  - 적극적으로 순환 피하기 (문제 해결책을 언제나 반영)
  - 함정을 감시하라 (의도적이고 악의적인 함정들)
  - 블랙리스트를 관리하라 (다시는 방문 X)
- 확장성
  - 공간 이해하기 (문제의 크기 미리 계산)
  - 대역폭 이해하기 (대역폭 실제 사용량 측정)
  - 시간 이해하기 (소요 시간 추정)
  - 분할 정복 (협력 시 더 많은 하드웨어 사용)
- 신뢰성
  - 철저하게 테스트하라 (사용 전 철저히 테스트)
  - 체크포인트 (체크포인트/재시작 기능 설계)
  - 실패에 대한 유연성 (실패 대비)
- 소통
  - 준비하라 (문의에 대한 응답 빠르게 할 수 있도록)
  - 이해하라 (로봇 차단 규칙 표준에 대해 설명)
  - 즉각 대응하라 (즉각적인 전문 대응)

## 9.6 검색엔진

- 웹 로봇을 가장 광범위하게 사용함

### 9.6.1 넓게 생각하라

- 검색엔진들은 오늘날 필수 & 꽤나 복잡해짐
- 수십억 개의 웹페이지들을 검색하기 위해 복잡한 크롤러 사용
- 대규모 크롤러가 많은 장비를 사용해서 요청을 병렬로 수행해야 함

### 9.6.2 현대적인 검색엔진의 아키텍처

- 검색엔진들은 갖고 있는 전 세계의 웹페이지들에 대해 '풀 텍스트 색인(full-text indexes)' 로컬 데이터베이스 생성
- 검색 엔진 크롤러들은 수집한 웹페이지를 풀 텍스트 색인에 추가 (스냅숏을 저장)
- 사용자들은 풀 텍스트 색인에 질의를 보냄

### 9.6.3 풀 텍스트 색인

- 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려주는 데이터베이스
- 각 단어를 포함한 문서들을 열거

### 9.6.4 질의 보내기

- 브라우저가 HTML 폼을 게이트웨이로 보냄
- 게이트웨이는 검색 질의 추출 후 풀 텍스트 색인을 검색 표현식으로 변환
- 웹 서버로 게이트웨이가 문서의 목록을 돌려주고 이를 HTML 페이지로 변환

### 9.6.5 검색 결과를 정렬하고 보여주기

- 검색엔진은 결과에 순위를 매기기 위한 알고리즘 사용
  - 관련도 랭킹 : 문서들 간의 순서
  - 크롤링 과정 중 수집된 통계 데이터를 사용

### 9.6.6 스푸핑

- 웹 마스터들의 속임수(특정 단어의 가짜 페이지, 키워드 나열 등등)를 잘 잡아내기 위해 알고리즘을 수정해야 함
